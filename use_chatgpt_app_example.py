# coding=utf-8
# Copyright 2023 South China University of Technology and 
# Engineering Research Ceter of Ministry of Education on Human Body Perception.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# Author: Chen Yirong <eeyirongchen@mail.scut.edu.cn>
# Date: 2023.03.14

''' è¿è¡Œæ–¹å¼

å®‰è£…ä¾èµ–
```bash
pip install tiktoken
pip install streamlit==1.27.0
```
å¯åŠ¨æœåŠ¡ï¼š
```bash
streamlit run use_chatgpt_app_v3.py --server.port 9026
```

## æµ‹è¯•è®¿é—®

http://116.57.86.151:9026

'''

# st-chat uses https://www.dicebear.com/styles for the avatar

# https://emoji6.com/emojiall/

import os
import json
import time
import tiktoken
import requests
import streamlit as st

dialogue_history_dir = './chatgpt_history'

#model_name="gpt-3.5-turbo-16k-0613"

url = "https://openai.api2d.net/v1/chat/completions"

headers = {
  'Content-Type': 'application/json',
  'Authorization': 'Bearer fkxxxxx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx' # <-- æŠŠ fkxxxxx æ›¿æ¢æˆä½ è‡ªå·±çš„ Forward Keyï¼Œæ³¨æ„å‰é¢çš„ Bearer è¦ä¿ç•™ï¼Œå¹¶ä¸”å’Œ Key ä¸­é—´æœ‰ä¸€ä¸ªç©ºæ ¼ã€‚
} # 1132461715@qq.com

st.set_page_config(
    page_title="ChatGPT(å†…ç½‘ç‰ˆ)",
    page_icon="ğŸ‘©â€ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'About': """     
-   ç‰ˆæœ¬ï¼šğŸ‘©â€ğŸ”¬ChatGPT(å†…ç½‘ç‰ˆ)
-   æœºæ„ï¼šå¹¿ä¸œçœæ•°å­—å­ªç”Ÿäººé‡ç‚¹å®éªŒå®¤
-   ä½œè€…ï¼šé™ˆè‰ºè£
	    """
    }
)

st.header("ğŸ‘©â€ğŸ”¬ChatGPT(å†…ç½‘ç‰ˆ)")

with st.expander("â„¹ï¸ - å…³äºæˆ‘ä»¬", expanded=False):
    st.write(
        """     
-   ç‰ˆæœ¬ï¼šğŸ‘©â€ğŸ”¬ChatGPT(å†…ç½‘ç‰ˆ)
-   æœºæ„ï¼šå¹¿ä¸œçœæ•°å­—å­ªç”Ÿäººé‡ç‚¹å®éªŒå®¤
-   ä½œè€…ï¼šé™ˆè‰ºè£
	    """
    )


def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0301"):
    """Returns the number of tokens used by a list of messages."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        print("Warning: model not found. Using cl100k_base encoding.")
        encoding = tiktoken.get_encoding("cl100k_base")
    if model == "gpt-3.5-turbo":
        print("Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.")
        return num_tokens_from_messages(messages, model="gpt-3.5-turbo-0301")
    elif model == "gpt-4":
        print("Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.")
        return num_tokens_from_messages(messages, model="gpt-4-0314")
    elif model == "gpt-3.5-turbo-0301":
        tokens_per_message = 4  # every message follows <|start|>{role/name}\n{content}<|end|>\n
        tokens_per_name = -1  # if there's a name, the role is omitted
    elif model == "gpt-4-0314":
        tokens_per_message = 3
        tokens_per_name = 1
    else:
        raise NotImplementedError(f"""num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.""")
    num_tokens = 0
    for message in messages:
        num_tokens += tokens_per_message
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":
                num_tokens += tokens_per_name
    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
    return num_tokens




def clear_chat_history():
    del st.session_state.messages


def init_chat_history():
    with st.chat_message("assistant", avatar='ğŸ‘©â€ğŸ”¬'):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯ChatGPT(å†…ç½‘ç‰ˆ),æœåŠ¡ç”±å¹¿ä¸œçœæ•°å­—å­ªç”Ÿäººé‡ç‚¹å®éªŒå®¤æä¾›ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

    if "messages" in st.session_state:
        for message in st.session_state.messages:
            avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ‘©â€ğŸ”¬'
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        st.session_state.messages = []

    return st.session_state.messages


def init_chat_id():
    if 'chatid' not in st.session_state:
        if not os.path.exists(dialogue_history_dir):
            # åˆ›å»ºä¿å­˜ç”¨æˆ·èŠå¤©è®°å½•çš„ç›®å½•
            os.makedirs(dialogue_history_dir)

        json_files = os.listdir(dialogue_history_dir)
        chat_id = len(json_files)
        st.session_state.chatid = chat_id
    
    return st.session_state.chatid
    



def get_history_chat_id():
    if not os.path.exists(dialogue_history_dir):
        # åˆ›å»ºä¿å­˜ç”¨æˆ·èŠå¤©è®°å½•çš„ç›®å½•
        os.makedirs(dialogue_history_dir)

    json_files = os.listdir(dialogue_history_dir)
    files = [int(os.path.splitext(file)[0]) for file in json_files]
    files = sorted(files, reverse=True)
    files = [str(file) for file in files]
    return files

    



def main():

    messages = init_chat_history()
    chat_id = init_chat_id()
    files = get_history_chat_id()
    files = [str(chat_id)] + files

    with st.sidebar:
        # AIåŠ©æ‰‹çš„ä¸ªæ€§è®¾ç½®
        model_name = st.selectbox(
            'è¯·é€‰æ‹©æ¨¡å‹çš„ç‰ˆæœ¬',
            ('gpt-3.5-turbo-16k-0613', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo', 'gpt-3.5-turbo-0301', 'gpt-4-0613', 'gpt-4'))
        history_chat_id = st.selectbox('è¯·é€‰æ‹©æŸ¥çœ‹çš„å†å²èŠå¤©è®°å½•', tuple(files))
        
    if history_chat_id != str(st.session_state.chatid):
        with open(os.path.join(dialogue_history_dir, str(history_chat_id)+'.json'), "r", encoding="utf-8") as f:
            messages = json.load(f)
        st.session_state.messages = messages
        for message in st.session_state.messages:
            avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ‘©â€ğŸ”¬'
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])

    if prompt := st.chat_input("Shift + Enter æ¢è¡Œ, Enter å‘é€"):
        with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
            st.markdown(prompt)
        messages.append({"role": "user", "content": prompt})
        print(f"[user] {prompt}", flush=True)
        with st.chat_message("assistant", avatar='ğŸ‘©â€ğŸ”¬'):
            placeholder = st.empty()
            data = {"model": model_name, "messages": messages, "stream": False}
            start_time = time.time()
            requests_result = requests.post(url, headers=headers, json=data)
            if requests_result.status_code==200:
                response = requests_result.json()['choices'][0]['message']["content"]


                if 'gpt-3.5' in model_name:
                    num_tokens_input_chatgpt = num_tokens_from_messages(data["messages"], model="gpt-3.5-turbo")
                    num_tokens_output_chatgpt = num_tokens_from_messages([{"role": "assistant", "content": response}], model="gpt-3.5-turbo")
                    if '16k' in model_name:
                        total_p = 10*num_tokens_input_chatgpt/666 + 10*num_tokens_output_chatgpt/500
                    else:
                        total_p = 10*num_tokens_input_chatgpt/1333 + 10*num_tokens_output_chatgpt/1000

                else:
                    num_tokens_input_chatgpt = num_tokens_from_messages(data["messages"], model="gpt-4")
                    num_tokens_output_chatgpt = num_tokens_from_messages([{"role": "assistant", "content": response}], model="gpt-4")
                    total_p = 10*num_tokens_input_chatgpt/66.6 + 10*num_tokens_output_chatgpt/33.3
                
                total_cost = 0.0021*total_p # 1050å…ƒå¯ä»¥è´­ä¹°500,000 p

                

            else:
                response = 'è¯·æ±‚å‡ºç°é”™è¯¯ï¼Œå¯èƒ½æ²¡æœ‰ç‚¹äº†ï¼Œè¯·å……å€¼åé‡è¯•ï¼Œæˆ–è€…ç½‘ç»œè¯·æ±‚å­˜åœ¨é—®é¢˜ï¼'

            end_time = time.time()
            total_time = start_time - end_time

            placeholder.markdown(response)

            with st.expander(label="*Related Information*"):
                st.write(
                    f"time=**{total_time:.2}s**, model_name=**{model_name}**, total_cost=**{total_cost:.2}å…ƒ**, total_p=**{total_p} P**"
            )

        messages.append({"role": "assistant", "content": response})
        print(json.dumps(messages, ensure_ascii=False), flush=True)

        if messages is not None:
            with open(os.path.join(dialogue_history_dir, str(st.session_state.chatid)+'.json'), "w", encoding="utf-8") as f:
                json.dump(messages, f, indent=4, ensure_ascii=False)

        st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)


if __name__ == "__main__":
    main()
